@article{hong2024odm,
  title={Overcoming Distribution Mismatch in Quantizing Image Super-Resolution Networks},
  author={Hong, Cheeun and Lee, Kyoung Mu},
  journal={European Conference on Computer Vision},
  abbr={ECCV},
  preview={cover_odm.png},
  pdf={https://arxiv.org/abs/2307.13337},
  code={https://github.com/Cheeun/ODM},
  selected={true},
  year={2024},
  abstract={Although quantization has emerged as a promising approach to reducing computational complexity across various high-level vision tasks, it inevitably leads to accuracy loss in image super-resolution (SR) networks. This is due to the significantly divergent feature distributions across different channels and input images of the SR networks, which complicates the selection of a fixed quantization range. Existing works address this distribution mismatch problem by dynamically adapting quantization ranges to the varying distributions during test time. However, such a dynamic adaptation incurs additional computational costs during inference. In contrast, we propose a new quantization-aware training scheme that effectively Overcomes the Distribution Mismatch problem in SR networks without the need for dynamic adaptation. Intuitively, this mismatch can be mitigated by regularizing the distance between the feature and a fixed quantization range. However, we observe that such regularization can conflict with the reconstruction loss during training, negatively impacting SR accuracy. Therefore, we opt to regularize the mismatch only when the gradients of the regularization are aligned with those of the reconstruction loss. Additionally, we introduce a layer-wise weight clipping correction scheme to determine a more suitable quantization range for layer-wise weights. Experimental results demonstrate that our framework effectively reduces the distribution mismatch and achieves state-of-the-art performance with minimal computational overhead. Codes are available at <a href="https://github.com/Cheeun/ODM">https://github.com/Cheeun/ODM</a>.},
}

@article{hong2024adabm,
  title={AdaBM: On-the-Fly Adaptive Bit Mapping for Image Super-Resolution},
  author={Hong, Cheeun and Lee, Kyoung Mu},
  journal={Conference on Computer Vision and Pattern Recognition},
  abbr={CVPR},
  preview={cover_adabm.png},
  pdf={https://arxiv.org/abs/2404.03296},
  code={https://github.com/Cheeun/AdaBM},
  selected={true},
  year={2024},
  abstract={Although image super-resolution (SR) problem has experienced unprecedented restoration accuracy with deep neural networks, it has yet limited versatile applications due to the substantial computational costs. Since different input images for SR face different restoration difficulties, adapting computation costs based on the input image, referred to as adaptive inference, has emerged as a promising solution to compress SR networks. Specifically, adaptively allocating quantization bit-widths has successfully reduced the inference and memory cost without sacrificing the accuracy. However, despite the benefits of the resultant network, existing works rely on time-intensive quantization-aware training with full access to the original training pairs, to learn appropriate bit allocation policies, which limits its ubiquitous usage. To this end, we introduce the first on-the-fly adaptive quantization framework that accelerates the processing time in hours to the processing level in seconds. We formulate the bit allocation problem with only two bit mapping modules: one to map input image to image-wise bit adaptation factor and one to obtain layer-wise adaptation factors. These bit mappings are calibrated and finetuned using only a small number of calibration images. We achieve competitive performance with the previous adaptive quantization methods, while the processing time is accelerated by x2400.}
}


@article{park2023colanet,
  title={CoLaNet: Adaptive Context and Latent Information Blending for Face Image Inpainting},
  author={Park, Joonkyu and Hong, Cheeun and Baik, Sungyong and Lee, Kyoung Mu},
  journal={IEEE Signal Processing Letters},
  preview={cover_colanet.png},
  abbr={IEEE SPL},
  selected={true},
  year={2023},
  abstract={Face inpainting, the task of filling up missing regions in a face image plausibly, has witnessed great advances with deep learning-based approaches. To fill in the missing region, existing methods either use information from the surrounding visible region of the input image itself (i.e., context) or use prior knowledge obtained from the training data (i.e., latent). However, we find that exclusive usage of the two types of information is sub-optimal; whether the context-based approach is effective or the latent-based approach is effective is different for each missing region. To this end, we propose CoLaNet, a novel framework that adaptively blends context and latent information to inpaint face images. Specifically, the two types of information are balanced based on the attention between the missing region and the rest of the image. The regions strongly correlated to the visible region leverage context information more. Consequently, the adaptive utilization of context and latent information leads to better inpainting performance in various face images.}
}


@article{hong2025ddpq,
  title={Difficulty, Plausibility, and Diversity: Dynamic Data-Free Quantization},
  author={Hong*, Cheeun and Baik*, Sungyong and Oh, Junghun and Lee, Kyoung Mu},
  journal={Winter Conference on Applications of Computer Vision},
  abbr={WACV},
  selected={true},
  year={2025},
  preview={cover_dynadfq.png},
  abstract={Without access to the original training data, data-free quantization (DFQ) aims to recover the performance loss induced by quantization. Most previous works have focused on using an original network to extract the train data information, which is instilled into surrogate synthesized images. However, existing DFQ methods do not take into account important aspects of quantization: the extent of a computational-cost-and-accuracy trade-off varies for each image depending on its task difficulty. Neglecting such aspects, previous works have resorted to the same-bit-width quantization. By contrast, without the original training data, we make dynamic quantization possible by modeling varying extents of task difficulties in synthesized data. To do so, we first note that networks are often confused with similar classes. Thus, we generate plausibly difficult images with soft labels, where the probabilities are allocated to a group of similar classes. Under data-free setting, we show that the class similarity information can be obtained from the similarities of corresponding weights in the classification layer. Using the class similarity, we generate plausible images of diverse difficulty levels, which enable us to train our framework to dynamically handle the varying trade-off. Consequently, we demonstrate that our first dynamic data-free quantization pipeline, dubbed DynaDFQ, achieves a better accuracy-complexity trade-off than existing data-free quantization approaches across various settings.}
}


@article{hong2022cadyq,
  title={CADyQ: Content-Aware Dynamic Quantization for Image Super-Resolution},
  author={Hong, Cheeun and Baik, Sungyong and Kim, Heewon and Nah, Seungjun and Lee, Kyoung Mu},
  journal={European Conference on Computer Vision},
  preview={cover_cadyq.png},
  pdf={https://arxiv.org/abs/2207.10345},
  selected={true},
  code={https://github.com/Cheeun/CADyQ},
  year={2022},
  abbr={ECCV},
  abstract={Despite breakthrough advances in image super-resolution (SR) with convolutional neural networks (CNNs), SR has yet to enjoy ubiquitous applications due to the high computational complexity of SR networks. Quantization is one of the promising approaches to solve this problem. However, existing methods fail to quantize SR models with a bit-width lower than 8 bits, suffering from severe accuracy loss due to fixed bit-width quantization applied everywhere. In this work, to achieve high average bit-reduction with less accuracy loss, we propose a novel Content-Aware Dynamic Quantization (CADyQ) method for SR networks that allocates optimal bits to local regions and layers adaptively based on the local contents of an input image. To this end, a trainable bit selector module is introduced to determine the proper bit-width and quantization level for each layer and a given local image patch. This module is governed by the quantization sensitivity that is estimated by using both the average magnitude of image gradient of the patch and the standard deviation of the input feature of the layer. The proposed quantization pipeline has been tested on various SR networks and evaluated on several standard benchmarks extensively. Significant reduction in computational complexity and the elevated restoration accuracy clearly demonstrate the effectiveness of the proposed CADyQ framework for SR. Codes are available at <a href="https://github.com/Cheeun/CADyQ">https://github.com/Cheeun/CADyQ</a>.}
}


@article{oh2022attentive,
  title={Attentive Fine-Grained Structured Sparsity for Image Restoration},
  author={Oh, Junghun and Kim, Heewon and Nah, Seungjun and Hong, Cheeun and Choi, Jonghyun and Lee, Kyoung Mu},
  journal={Conference on Computer Vision and Pattern Recognition},
  preview={cover_sls.jpg},
  pdf={https://openaccess.thecvf.com/content/CVPR2022/papers/Oh_Attentive_Fine-Grained_Structured_Sparsity_for_Image_Restoration_CVPR_2022_paper.pdf},
  selected={true},
  code={https://github.com/JungHunOh/SLS_CVPR2022},
  abbr={CVPR},
  year={2022},
  abstract={Image restoration tasks have witnessed great performance improvement in recent years by developing large deep models. Despite the outstanding performance, the heavy computation demanded by the deep models has restricted the application of image restoration. To lift the restriction, it is required to reduce the size of the networks while maintaining accuracy. Recently, N: M structured pruning has appeared as one of the effective and practical pruning approaches for making the model efficient with the accuracy constraint. However, it fails to account for different computational complexities and performance requirements for different layers of an image restoration network. To further optimize the trade-off between the efficiency and the restoration accuracy, we propose a novel pruning method that determines the pruning ratio for N: M structured sparsity at each layer. Extensive experimental results on super-resolution and deblurring tasks demonstrate the efficacy of our method which outperforms previous pruning methods significantly. PyTorch implementation for the proposed methods will be publicly available at <a href="https://github.com/JungHunOh/SLS_CVPR2022">https://github.com/JungHunOh/SLS_CVPR2022</a>.}
}

@article{hong2022daq,
  title={DAQ: Channel-Wise Distribution-Aware Quantization for Deep Image Super-Resolution Networks},
  author={Hong*, Cheeun and Kim*, Heewon and Baik, Sungyong and Oh, Junghun and Lee, Kyoung Mu},
  journal={Winter Conference on Applications of Computer Vision},
  preview={cover_daq.png},
  pdf={https://cv.snu.ac.kr/publication/conf/2022/wacv2022_daq.pdf},
  selected={true},
  code={https://github.com/Cheeun/DAQ-pytorch},
  abbr={WACV},
  year={2022},
  abstract={Since the resurgence of deep neural networks (DNNs), image super-resolution (SR) has recently seen a huge progress in improving the quality of low resolution images, however at the great cost of computations and resources. Recently, there has been several efforts to make DNNs more efficient via quantization. However, SR demands pixel-level accuracy in the system, it is more difficult to perform quantization without significantly sacrificing SR performance. To this end, we introduce a new ultra-low precision yet effective quantization approach specifically designed for SR. In particular, we observe that in recent SR networks, each channel has different distribution characteristics. Thus we propose a channel-wise distribution-aware quantization scheme. Experimental results demonstrate that our proposed quantization, dubbed Distribution-Aware Quantization (DAQ), manages to greatly reduce the computational and resource costs without the significant sacrifice in SR performance, compared to other quantization methods.}
}

@article{oh2022batch,
  title={Batch Normalization Tells You Which Filter is Important},
  author={Oh, Junghun and Kim, Heewon and Baik, Sungyong and Hong, Cheeun and Lee, Kyoung Mu},
  journal={Winter Conference on Applications of Computer Vision},
  preview={cover_bnfi.png},
  pdf={https://arxiv.org/abs/2112.01155},
  selected={true},
  abbr={WACV},
  year={2022},
  abstract={The goal of filter pruning is to search for unimportant filters to remove in order to make convolutional neural networks (CNNs) efficient without sacrificing the performance in the process. The challenge lies in finding information that can help determine how important or relevant each filter is with respect to the final output of neural networks. In this work, we share our observation that the batch normalization (BN) parameters of pre-trained CNNs can be used to estimate the feature distribution of activation outputs, without processing of training data. Upon observation, we propose a simple yet effective filter pruning method by evaluating the importance of each filter based on the BN parameters of pre-trained CNNs. The experimental results on CIFAR-10 and ImageNet demonstrate that the proposed method can achieve outstanding performance with and without fine-tuning in terms of the trade-off between the accuracy drop and the reduction in computational complexity and number of parameters of pruned networks.}
}

