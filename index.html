<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  Cheeun Hong


</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ’œ</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="https://cheeunhong.github.io/">
        Cheeun Hong
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                Publications
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/others/">
                Awards & Services
                
              </a>
          </li>
          
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">
  <article>
    <div class="row">
        <div class="col-sm-3">
            
            <img style="width: 170px;border-radius: 170px" src="/assets/img/cheeun_pic_crop.jpg">
            
        </div>
    
        <div class="col-sm-9">
            <h1 class="post-title">
            Cheeun Hong
            </h1>
            <p class="desc"><p>Ph.D. Candidate @ Computer Vision Lab<br>Department of Electrical and Computer Engineering, Seoul National University</p>
</p>
            <div class="social">
                <div class="contact-icons">
                <a href="mailto:%63%68%65%65%75%6E%39%31%34@%73%6E%75.%61%63.%6B%72"><i class="fas fa-envelope"></i></a>
<a href="assets/pdf/cheeunhong-cv-2025.11.pdf"><i class="ai ai-cv"></i></a>

<a href="https://scholar.google.com/citations?user=HHjMKhIAAAAJ" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>


<a href="https://github.com/Cheeun" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
<a href="https://www.linkedin.com/in/cheeun-hong-6074b2176" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a>











                </div>
            </div>
   
        </div>
    </div>

    <div class="clearfix">
      <p>I am passionate about advancing <strong>efficient AI</strong> to optimize both model training and inference, with the ultimate goal of promoting sustainable AI. My research focuses on developing cutting-edge techniques like <strong>network quantization</strong>, <strong>pruning</strong>, and <strong>test-time adaptation</strong>, aimed at drastically reducing the computational costs while maintaining high performance. While much of my work has targeted efficiency improvements in low-level vision tasks like image restoration, my broader goal is to compress large-scale, computationally intensive modelsâ€”including vision-language and generative modelsâ€”to move closer to achieving <strong>on-device AI</strong>. <br /></p>

<p style="text-align: right;">Keywords: Efficient AI, On-device AI</p>

    </div>


    
      <div class="news">
  <h2>Recent News</h2>
  
    <div class="table-responsive">
      <table class="table table-sm table-borderless">
      
      
        <tr>
          <!-- <th scope="row">Jun 1, 2025</th> -->
          <th scope="row">Jun 2025</th>
          <td>
            
              I will be working as a Research Scientist Intern in <code class="language-plaintext highlighter-rouge">Meta SuperIntelligence Labs</code> for 6 months!


            
          </td>
        </tr>
      
        <tr>
          <!-- <th scope="row">Oct 29, 2024</th> -->
          <th scope="row">Oct 2024</th>
          <td>
            
              One first-co-authored paper on data-free quantization, <code class="language-plaintext highlighter-rouge">DDPQ</code> got accepted to WACV 2025.

            
          </td>
        </tr>
      
        <tr>
          <!-- <th scope="row">Jul 2, 2024</th> -->
          <th scope="row">Jul 2024</th>
          <td>
            
              One first-authored paper on quantization, <code class="language-plaintext highlighter-rouge">ODM</code> got accepted to ECCV 2024. See you in Milano!

            
          </td>
        </tr>
      
        <tr>
          <!-- <th scope="row">Feb 27, 2024</th> -->
          <th scope="row">Feb 2024</th>
          <td>
            
              One first-authored paper on quantization, <code class="language-plaintext highlighter-rouge">AdaBM</code> got accepted to CVPR 2024. See you in Seattle!

            
          </td>
        </tr>
      
      </table>
    </div>
  
</div>

    

    
      <!-- papers -->
<div class="publications">
  <h2>Selected Publications</h2>
  <ol class="bibliography"><li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3 abbr"><div class="preview">
              <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img 
      src="/assets/img/publication_preview/cover_sneakpeek.png"
      class="preview img-fluid z-depth-1"  
      width="350" 
      height="auto" 
       
       
       
      height="170" 
      alt="cover_sneakpeek.png" 
       
      
      onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
    />

  </picture>

</figure></div></div>
        
        <!-- Entry bib key -->
        <div id="hong2025sneakpeek" class="col-sm-9">
        <!-- Title -->
        <div class="title">SneakPeek: Future-Guided Instructional Streaming Video Generation</div>
        <!-- Author -->
        <div class="author">
        

        <em>Cheeun Hong</em>,&nbsp;German Barquero,&nbsp;Fadime Sener,&nbsp;Markos Georgopoulos,&nbsp;Edgar SchÃ¶nfeld,&nbsp;Stefan Popov,&nbsp;Yuming Du,&nbsp;Oscar MaÃ±as,&nbsp;and&nbsp;Albert Pumarola</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In ArXiv preprint</em> (<b></b>), 2025
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
          <!-- -->
            <a href="https://arxiv.org/pdf/2512.13019" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>
          
          <div class="badges">
              <span class="__dimensions_badge_embed__"
              
                data-pmid=""
              
              data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>
          <!-- tldr here? -->
          <!-- -->

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Instructional video generation is an emerging task that aims to synthesize coherent demonstrations of procedural activities from textual descriptions. Such capability has broad implications for content creation, education, and human-AI interaction, yet existing video diffusion models struggle to maintain temporal consistency and controllability across long sequences of multiple action steps. We introduce a pipeline for future-driven streaming instructional video generation, dubbed SneakPeek, a diffusion-based autoregressive framework designed to generate precise, stepwise instructional videos conditioned on an initial image and structured textual prompts. Our approach introduces three key innovations to enhance consistency and controllability: (1) predictive causal adaptation, where a causal model learns to perform next-frame prediction and anticipate future keyframes; (2) future-guided self-forcing with a dual-region KV caching scheme to address the exposure bias issue at inference time; (3) multi-prompt conditioning, which provides fine-grained and procedural control over multi-step instructions. Together, these components mitigate temporal drift, preserve motion consistency, and enable interactive video generation where future prompt updates dynamically influence ongoing streaming video generation. Experimental results demonstrate that our method produces temporally coherent and semantically faithful instructional videos that accurately follow complex, multi-step task descriptions.</p>
          </div><!-- -->


          
        </div>
      </div></li>
<li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3 abbr"><div class="preview">
              <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img 
      src="/assets/img/publication_preview/cover_dynadfq.png"
      class="preview img-fluid z-depth-1"  
      width="350" 
      height="auto" 
       
       
       
      height="170" 
      alt="cover_dynadfq.png" 
       
      
      onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
    />

  </picture>

</figure></div><div class="abbr"><abbr class="badge">WACV</abbr></div></div>
        
        <!-- Entry bib key -->
        <div id="hong2025ddpq" class="col-sm-9">
        <!-- Title -->
        <div class="title">Difficulty, Plausibility, and Diversity: Dynamic Data-Free Quantization</div>
        <!-- Author -->
        <div class="author">
        

        <em>Cheeun Hong*</em>,&nbsp;Sungyong Baik*,&nbsp;Junghun Oh,&nbsp;and&nbsp;Kyoung Mu Lee</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Winter Conference on Applications of Computer Vision</em> (<b>WACV</b>), 2025
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
          <!-- -->
            <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10943763" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>
          
          <div class="badges">
              <span class="__dimensions_badge_embed__"
              
                data-pmid=""
              
              data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>
          <!-- tldr here? -->
          <!-- -->

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Without access to the original training data, data-free quantization (DFQ) aims to recover the performance loss induced by quantization. Most previous works have focused on using an original network to extract the train data information, which is instilled into surrogate synthesized images. However, existing DFQ methods do not take into account important aspects of quantization: the extent of a computational-cost-and-accuracy trade-off varies for each image depending on its task difficulty. Neglecting such aspects, previous works have resorted to the same-bit-width quantization. By contrast, without the original training data, we make dynamic quantization possible by modeling varying extents of task difficulties in synthesized data. To do so, we first note that networks are often confused with similar classes. Thus, we generate plausibly difficult images with soft labels, where the probabilities are allocated to a group of similar classes. Under data-free setting, we show that the class similarity information can be obtained from the similarities of corresponding weights in the classification layer. Using the class similarity, we generate plausible images of diverse difficulty levels, which enable us to train our framework to dynamically handle the varying trade-off. Consequently, we demonstrate that our first dynamic data-free quantization pipeline, dubbed DynaDFQ, achieves a better accuracy-complexity trade-off than existing data-free quantization approaches across various settings.</p>
          </div><!-- -->


          
        </div>
      </div></li>
<li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3 abbr"><div class="preview">
              <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img 
      src="/assets/img/publication_preview/cover_odm.png"
      class="preview img-fluid z-depth-1"  
      width="350" 
      height="auto" 
       
       
       
      height="170" 
      alt="cover_odm.png" 
       
      
      onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
    />

  </picture>

</figure></div><div class="abbr"><abbr class="badge">ECCV</abbr></div></div>
        
        <!-- Entry bib key -->
        <div id="hong2024odm" class="col-sm-9">
        <!-- Title -->
        <div class="title">Overcoming Distribution Mismatch in Quantizing Image Super-Resolution Networks</div>
        <!-- Author -->
        <div class="author">
        

        <em>Cheeun Hong</em>,&nbsp;and&nbsp;Kyoung Mu Lee</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In European Conference on Computer Vision</em> (<b>ECCV</b>), 2024
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
          <!-- -->
            <a href="https://arxiv.org/abs/2307.13337" class="btn btn-sm z-depth-0" role="button">PDF</a>
            <a href="https://github.com/Cheeun/ODM" class="btn btn-sm z-depth-0" role="button">Code</a>
          </div>
          
          <div class="badges">
              <span class="__dimensions_badge_embed__"
              
                data-pmid=""
              
              data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>
          <!-- tldr here? -->
          <!-- -->

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Although quantization has emerged as a promising approach to reducing computational complexity across various high-level vision tasks, it inevitably leads to accuracy loss in image super-resolution (SR) networks. This is due to the significantly divergent feature distributions across different channels and input images of the SR networks, which complicates the selection of a fixed quantization range. Existing works address this distribution mismatch problem by dynamically adapting quantization ranges to the varying distributions during test time. However, such a dynamic adaptation incurs additional computational costs during inference. In contrast, we propose a new quantization-aware training scheme that effectively Overcomes the Distribution Mismatch problem in SR networks without the need for dynamic adaptation. Intuitively, this mismatch can be mitigated by regularizing the distance between the feature and a fixed quantization range. However, we observe that such regularization can conflict with the reconstruction loss during training, negatively impacting SR accuracy. Therefore, we opt to regularize the mismatch only when the gradients of the regularization are aligned with those of the reconstruction loss. Additionally, we introduce a layer-wise weight clipping correction scheme to determine a more suitable quantization range for layer-wise weights. Experimental results demonstrate that our framework effectively reduces the distribution mismatch and achieves state-of-the-art performance with minimal computational overhead. Codes are available at <a href="https://github.com/Cheeun/ODM">https://github.com/Cheeun/ODM</a>.</p>
          </div><!-- -->


          
        </div>
      </div></li>
<li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3 abbr"><div class="preview">
              <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img 
      src="/assets/img/publication_preview/cover_adabm.png"
      class="preview img-fluid z-depth-1"  
      width="350" 
      height="auto" 
       
       
       
      height="170" 
      alt="cover_adabm.png" 
       
      
      onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
    />

  </picture>

</figure></div><div class="abbr"><abbr class="badge">CVPR</abbr></div></div>
        
        <!-- Entry bib key -->
        <div id="hong2024adabm" class="col-sm-9">
        <!-- Title -->
        <div class="title">AdaBM: On-the-Fly Adaptive Bit Mapping for Image Super-Resolution</div>
        <!-- Author -->
        <div class="author">
        

        <em>Cheeun Hong</em>,&nbsp;and&nbsp;Kyoung Mu Lee</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Conference on Computer Vision and Pattern Recognition</em> (<b>CVPR</b>), 2024
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
          <!-- -->
            <a href="https://arxiv.org/abs/2404.03296" class="btn btn-sm z-depth-0" role="button">PDF</a>
            <a href="https://github.com/Cheeun/AdaBM" class="btn btn-sm z-depth-0" role="button">Code</a>
          </div>
          
          <div class="badges">
              <span class="__dimensions_badge_embed__"
              
                data-pmid=""
              
              data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>
          <!-- tldr here? -->
          <!-- -->

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Although image super-resolution (SR) problem has experienced unprecedented restoration accuracy with deep neural networks, it has yet limited versatile applications due to the substantial computational costs. Since different input images for SR face different restoration difficulties, adapting computation costs based on the input image, referred to as adaptive inference, has emerged as a promising solution to compress SR networks. Specifically, adaptively allocating quantization bit-widths has successfully reduced the inference and memory cost without sacrificing the accuracy. However, despite the benefits of the resultant network, existing works rely on time-intensive quantization-aware training with full access to the original training pairs, to learn appropriate bit allocation policies, which limits its ubiquitous usage. To this end, we introduce the first on-the-fly adaptive quantization framework that accelerates the processing time in hours to the processing level in seconds. We formulate the bit allocation problem with only two bit mapping modules: one to map input image to image-wise bit adaptation factor and one to obtain layer-wise adaptation factors. These bit mappings are calibrated and finetuned using only a small number of calibration images. We achieve competitive performance with the previous adaptive quantization methods, while the processing time is accelerated by x2400.</p>
          </div><!-- -->


          
        </div>
      </div></li>
<li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3 abbr"><div class="preview">
              <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img 
      src="/assets/img/publication_preview/cover_colanet.png"
      class="preview img-fluid z-depth-1"  
      width="350" 
      height="auto" 
       
       
       
      height="170" 
      alt="cover_colanet.png" 
       
      
      onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
    />

  </picture>

</figure></div><div class="abbr"><abbr class="badge">IEEE SPL</abbr></div></div>
        
        <!-- Entry bib key -->
        <div id="park2023colanet" class="col-sm-9">
        <!-- Title -->
        <div class="title">CoLaNet: Adaptive Context and Latent Information Blending for Face Image Inpainting</div>
        <!-- Author -->
        <div class="author">
        

        Joonkyu Park,&nbsp;<em>Cheeun Hong</em>,&nbsp;Sungyong Baik,&nbsp;and&nbsp;Kyoung Mu Lee</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In IEEE Signal Processing Letters</em> (<b>IEEE SPL</b>), 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
          <!-- -->
            <a href="https://ieeexplore.ieee.org/document/10349927" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>
          
          <div class="badges">
              <span class="__dimensions_badge_embed__"
              
                data-pmid=""
              
              data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>
          <!-- tldr here? -->
          <!-- -->

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Face inpainting, the task of filling up missing regions in a face image plausibly, has witnessed great advances with deep learning-based approaches. To fill in the missing region, existing methods either use information from the surrounding visible region of the input image itself (i.e., context) or use prior knowledge obtained from the training data (i.e., latent). However, we find that exclusive usage of the two types of information is sub-optimal; whether the context-based approach is effective or the latent-based approach is effective is different for each missing region. To this end, we propose CoLaNet, a novel framework that adaptively blends context and latent information to inpaint face images. Specifically, the two types of information are balanced based on the attention between the missing region and the rest of the image. The regions strongly correlated to the visible region leverage context information more. Consequently, the adaptive utilization of context and latent information leads to better inpainting performance in various face images.</p>
          </div><!-- -->


          
        </div>
      </div></li>
<li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3 abbr"><div class="preview">
              <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img 
      src="/assets/img/publication_preview/cover_cadyq.png"
      class="preview img-fluid z-depth-1"  
      width="350" 
      height="auto" 
       
       
       
      height="170" 
      alt="cover_cadyq.png" 
       
      
      onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
    />

  </picture>

</figure></div><div class="abbr"><abbr class="badge">ECCV</abbr></div></div>
        
        <!-- Entry bib key -->
        <div id="hong2022cadyq" class="col-sm-9">
        <!-- Title -->
        <div class="title">CADyQ: Content-Aware Dynamic Quantization for Image Super-Resolution</div>
        <!-- Author -->
        <div class="author">
        

        <em>Cheeun Hong</em>,&nbsp;Sungyong Baik,&nbsp;Heewon Kim,&nbsp;Seungjun Nah,&nbsp;and&nbsp;Kyoung Mu Lee</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In European Conference on Computer Vision</em> (<b>ECCV</b>), 2022
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
          <!-- -->
            <a href="https://arxiv.org/abs/2207.10345" class="btn btn-sm z-depth-0" role="button">PDF</a>
            <a href="https://github.com/Cheeun/CADyQ" class="btn btn-sm z-depth-0" role="button">Code</a>
          </div>
          
          <div class="badges">
              <span class="__dimensions_badge_embed__"
              
                data-pmid=""
              
              data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>
          <!-- tldr here? -->
          <!-- -->

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Despite breakthrough advances in image super-resolution (SR) with convolutional neural networks (CNNs), SR has yet to enjoy ubiquitous applications due to the high computational complexity of SR networks. Quantization is one of the promising approaches to solve this problem. However, existing methods fail to quantize SR models with a bit-width lower than 8 bits, suffering from severe accuracy loss due to fixed bit-width quantization applied everywhere. In this work, to achieve high average bit-reduction with less accuracy loss, we propose a novel Content-Aware Dynamic Quantization (CADyQ) method for SR networks that allocates optimal bits to local regions and layers adaptively based on the local contents of an input image. To this end, a trainable bit selector module is introduced to determine the proper bit-width and quantization level for each layer and a given local image patch. This module is governed by the quantization sensitivity that is estimated by using both the average magnitude of image gradient of the patch and the standard deviation of the input feature of the layer. The proposed quantization pipeline has been tested on various SR networks and evaluated on several standard benchmarks extensively. Significant reduction in computational complexity and the elevated restoration accuracy clearly demonstrate the effectiveness of the proposed CADyQ framework for SR. Codes are available at <a href="https://github.com/Cheeun/CADyQ">https://github.com/Cheeun/CADyQ</a>.</p>
          </div><!-- -->


          
        </div>
      </div></li>
<li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3 abbr"><div class="preview">
              <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img 
      src="/assets/img/publication_preview/cover_sls.jpg"
      class="preview img-fluid z-depth-1"  
      width="350" 
      height="auto" 
       
       
       
      height="170" 
      alt="cover_sls.jpg" 
       
      
      onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
    />

  </picture>

</figure></div><div class="abbr"><abbr class="badge">CVPR</abbr></div></div>
        
        <!-- Entry bib key -->
        <div id="oh2022attentive" class="col-sm-9">
        <!-- Title -->
        <div class="title">Attentive Fine-Grained Structured Sparsity for Image Restoration</div>
        <!-- Author -->
        <div class="author">
        

        Junghun Oh,&nbsp;Heewon Kim,&nbsp;Seungjun Nah,&nbsp;<em>Cheeun Hong</em>,&nbsp;Jonghyun Choi,&nbsp;and&nbsp;Kyoung Mu Lee</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Conference on Computer Vision and Pattern Recognition</em> (<b>CVPR</b>), 2022
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
          <!-- -->
            <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Oh_Attentive_Fine-Grained_Structured_Sparsity_for_Image_Restoration_CVPR_2022_paper.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
            <a href="https://github.com/JungHunOh/SLS_CVPR2022" class="btn btn-sm z-depth-0" role="button">Code</a>
          </div>
          
          <div class="badges">
              <span class="__dimensions_badge_embed__"
              
                data-pmid=""
              
              data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>
          <!-- tldr here? -->
          <!-- -->

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Image restoration tasks have witnessed great performance improvement in recent years by developing large deep models. Despite the outstanding performance, the heavy computation demanded by the deep models has restricted the application of image restoration. To lift the restriction, it is required to reduce the size of the networks while maintaining accuracy. Recently, N: M structured pruning has appeared as one of the effective and practical pruning approaches for making the model efficient with the accuracy constraint. However, it fails to account for different computational complexities and performance requirements for different layers of an image restoration network. To further optimize the trade-off between the efficiency and the restoration accuracy, we propose a novel pruning method that determines the pruning ratio for N: M structured sparsity at each layer. Extensive experimental results on super-resolution and deblurring tasks demonstrate the efficacy of our method which outperforms previous pruning methods significantly. PyTorch implementation for the proposed methods will be publicly available at <a href="https://github.com/JungHunOh/SLS_CVPR2022">https://github.com/JungHunOh/SLS_CVPR2022</a>.</p>
          </div><!-- -->


          
        </div>
      </div></li>
<li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3 abbr"><div class="preview">
              <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img 
      src="/assets/img/publication_preview/cover_daq.png"
      class="preview img-fluid z-depth-1"  
      width="350" 
      height="auto" 
       
       
       
      height="170" 
      alt="cover_daq.png" 
       
      
      onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
    />

  </picture>

</figure></div><div class="abbr"><abbr class="badge">WACV</abbr></div></div>
        
        <!-- Entry bib key -->
        <div id="hong2022daq" class="col-sm-9">
        <!-- Title -->
        <div class="title">DAQ: Channel-Wise Distribution-Aware Quantization for Deep Image Super-Resolution Networks</div>
        <!-- Author -->
        <div class="author">
        

        <em>Cheeun Hong*</em>,&nbsp;Heewon Kim*,&nbsp;Sungyong Baik,&nbsp;Junghun Oh,&nbsp;and&nbsp;Kyoung Mu Lee</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Winter Conference on Applications of Computer Vision</em> (<b>WACV</b>), 2022
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
          <!-- -->
            <a href="https://cv.snu.ac.kr/publication/conf/2022/wacv2022_daq.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
            <a href="https://github.com/Cheeun/DAQ-pytorch" class="btn btn-sm z-depth-0" role="button">Code</a>
          </div>
          
          <div class="badges">
              <span class="__dimensions_badge_embed__"
              
                data-pmid=""
              
              data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>
          <!-- tldr here? -->
          <!-- -->

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Since the resurgence of deep neural networks (DNNs), image super-resolution (SR) has recently seen a huge progress in improving the quality of low resolution images, however at the great cost of computations and resources. Recently, there has been several efforts to make DNNs more efficient via quantization. However, SR demands pixel-level accuracy in the system, it is more difficult to perform quantization without significantly sacrificing SR performance. To this end, we introduce a new ultra-low precision yet effective quantization approach specifically designed for SR. In particular, we observe that in recent SR networks, each channel has different distribution characteristics. Thus we propose a channel-wise distribution-aware quantization scheme. Experimental results demonstrate that our proposed quantization, dubbed Distribution-Aware Quantization (DAQ), manages to greatly reduce the computational and resource costs without the significant sacrifice in SR performance, compared to other quantization methods.</p>
          </div><!-- -->


          
        </div>
      </div></li>
<li><!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-3 abbr"><div class="preview">
              <figure>

  <picture>
    

    <!-- Fallback to the original file -->
    <img 
      src="/assets/img/publication_preview/cover_bnfi.png"
      class="preview img-fluid z-depth-1"  
      width="350" 
      height="auto" 
       
       
       
      height="170" 
      alt="cover_bnfi.png" 
       
      
      onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
    />

  </picture>

</figure></div><div class="abbr"><abbr class="badge">WACV</abbr></div></div>
        
        <!-- Entry bib key -->
        <div id="oh2022batch" class="col-sm-9">
        <!-- Title -->
        <div class="title">Batch Normalization Tells You Which Filter is Important</div>
        <!-- Author -->
        <div class="author">
        

        Junghun Oh,&nbsp;Heewon Kim,&nbsp;Sungyong Baik,&nbsp;<em>Cheeun Hong</em>,&nbsp;and&nbsp;Kyoung Mu Lee</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In Winter Conference on Applications of Computer Vision</em> (<b>WACV</b>), 2022
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
          <!-- -->
            <a href="https://arxiv.org/abs/2112.01155" class="btn btn-sm z-depth-0" role="button">PDF</a>
          </div>
          
          <div class="badges">
              <span class="__dimensions_badge_embed__"
              
                data-pmid=""
              
              data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>
          <!-- tldr here? -->
          <!-- -->

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>The goal of filter pruning is to search for unimportant filters to remove in order to make convolutional neural networks (CNNs) efficient without sacrificing the performance in the process. The challenge lies in finding information that can help determine how important or relevant each filter is with respect to the final output of neural networks. In this work, we share our observation that the batch normalization (BN) parameters of pre-trained CNNs can be used to estimate the feature distribution of activation outputs, without processing of training data. Upon observation, we propose a simple yet effective filter pruning method by evaluating the importance of each filter based on the BN parameters of pre-trained CNNs. The experimental results on CIFAR-10 and ImageNet demonstrate that the proposed method can achieve outstanding performance with and without fine-tuning in terms of the trade-off between the accuracy drop and the reduction in computational complexity and number of parameters of pruned networks.</p>
          </div><!-- -->


          
        </div>
      </div></li></ol>
</div>

    

    
      <div class="education">
  <h2>Education</h2>
  
    <div class="table-responsive">
      <table class="table table-sm table-borderless">
        
        
            <tr>
              <th scope="row">Mar 2020</th>
              
                  <th scope="row"></th>
              
              <td>
                
                  Seoul National University, South Korea<br /> Integrated Ph.D. in Electrical and Computer Engineering<br /> Advisor: Prof. <a href="https://cv.snu.ac.kr/index.php/kmlee/">Kyoung Mu Lee</a> <br />

                
                <br>
              </td>
            </tr>
        
            <tr>
              <th scope="row">Mar 2015</th>
              
                  <th scope="row">- Feb 2020</th>
              
              <td>
                
                  Seoul National University, South Korea<br /> B.S. in Electrical and Computer Engineering<br />

                
                <br>
              </td>
            </tr>
        
      </table>
    </div>
  
</div>

    

    
      <div class="internship">
  <h2>Internship</h2>
  
    <div class="table-responsive">
      <table class="table table-sm table-borderless">
        
        
            <tr>
              <th scope="row">Jun 2025</th>
              
                  <th scope="row">- Dec 2025</th>
              
              <td>
                
                  <em>Research Scientist Intern</em> @ <code class="language-plaintext highlighter-rouge">Meta SuperIntelligence Labs (MSL)</code>, Meta, Switzerand <br />
Worked on post-training for efficient video generation model

                
                <br>
              </td>
            </tr>
        
            <tr>
              <th scope="row">Jun 2019</th>
              
                  <th scope="row">- Aug 2019</th>
              
              <td>
                
                  <em>Student Research Intern</em> @ <code class="language-plaintext highlighter-rouge">Machine Intelligence and Pattern Analysis Lab (MIPAL)</code>, South Korea <br />
Mentor: Prof. Nojun Kwak

                
                <br>
              </td>
            </tr>
        
            <tr>
              <th scope="row">Jun 2018</th>
              
                  <th scope="row">- Aug 2018</th>
              
              <td>
                
                  <em>Engineering Intern</em> @ <code class="language-plaintext highlighter-rouge">SK Hynix</code>, South Korea <br />
Worked in DRAM circuit design team on efficient verification

                
                <br>
              </td>
            </tr>
        
      </table>
    </div>
  
</div>

    


    
  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="sticky-bottom mt-5">
  <div class="container">
    &copy; Copyright 2025 Cheeun  Hong.
    
    
    
    Last updated: December 24, 2025.
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
